{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "yCe0YKQ4xnjE",
        "outputId": "35bdf700-2b88-4762-a2dd-609aaf73e6fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"import zipfile\n",
        "\n",
        "# Path to the Akhbarona zip file\n",
        "akhbarona_zip_path = '/content/drive/MyDrive/Arabic_Sentiment_Tweets/Akhbarona.zip'\n",
        "\n",
        "# Extract the Akhbarona dataset\n",
        "with zipfile.ZipFile(akhbarona_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/drive/MyDrive/Arabic_Sentiment_Tweets/Akhbarona')\n"
      ],
      "metadata": {
        "id": "6gFQmw4h0i9h",
        "outputId": "23a4192e-18d4-499b-be0c-7b1307f9794a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-2-b1aa9b147e07>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-b1aa9b147e07>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    \"\"\"import zipfile\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Path to the Arabiya zip file\n",
        "arabiya_zip_path = '/content/drive/MyDrive/Arabic_Sentiment_Tweets/Arabiya.zip'\n",
        "\n",
        "# Extract the Arabiya dataset\n",
        "with zipfile.ZipFile(arabiya_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/drive/MyDrive/Arabic_Sentiment_Tweets/Arabiya')\n"
      ],
      "metadata": {
        "id": "zVQHVkAI0vf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Path to the Khaleej zip file\n",
        "khaleej_zip_path = '/content/drive/MyDrive/Arabic_Sentiment_Tweets/Khaleej.zip'\n",
        "\n",
        "# Extract the Khaleej dataset\n",
        "with zipfile.ZipFile(khaleej_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/drive/MyDrive/Arabic_Sentiment_Tweets/Khaleej')\n"
      ],
      "metadata": {
        "id": "1E0D1rFd1Eba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Path to the SANAD_SUBSET zip file\n",
        "sanad_subset_zip_path = '/content/drive/MyDrive/Arabic_Sentiment_Tweets/SANAD_SUBSET.zip'\n",
        "\n",
        "# Extract the SANAD_SUBSET dataset\n",
        "with zipfile.ZipFile(sanad_subset_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/drive/MyDrive/Arabic_Sentiment_Tweets/SANAD_SUBSET')\n"
      ],
      "metadata": {
        "id": "PLZmBkBh1Igc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"import os\n",
        "\n",
        "# Path to the dataset folder in Google Drive\n",
        "dataset_path = '/content/drive/MyDrive/Arabic_Sentiment_Tweets'\n",
        "\n",
        "# List all files in the directory\n",
        "dataset_files = os.listdir(dataset_path)\n",
        "\n",
        "# Print the list of files\n",
        "for file in dataset_files:\n",
        "    print(file)\n"
      ],
      "metadata": {
        "id": "D9MWUBpgnwRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# List the contents of each folder\n",
        "for folder in dataset_files:\n",
        "    folder_path = os.path.join(dataset_path, folder)\n",
        "    if os.path.isdir(folder_path):  # Check if it's a directory\n",
        "        print(f\"Files in {folder}:\")\n",
        "        files_in_folder = os.listdir(folder_path)\n",
        "        for file in files_in_folder:\n",
        "            print(file)\n",
        "    else:\n",
        "        print(f\"{folder} is a file, not a directory.\")\n"
      ],
      "metadata": {
        "id": "E-lCfeKpqVDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Path to a sample text file (e.g., one of the .txt files)\n",
        "sample_file_path = '/content/drive/MyDrive/Arabic_Sentiment_Tweets/Akhbarona/Culture/5002.txt'\n",
        "\n",
        "# Open the file and read its contents\n",
        "with open(sample_file_path, 'r', encoding='utf-8') as file:\n",
        "    sample_data = file.read()\n",
        "\n",
        "# Print the first 500 characters to inspect the content\n",
        "print(sample_data[:500])\n"
      ],
      "metadata": {
        "id": "hcQX5QG7qVpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch torchmetrics\n"
      ],
      "metadata": {
        "id": "dD1EbDE71M1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea0d912-d711-4617-e3d6-56b8c0a88bf2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "# Load tokenizer\n",
        "model_name = \"aubmindlab/bert-base-arabertv2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load your dataset here\n",
        "texts = [\"Arabic text example 1\", \"Arabic text example 2\"]  # Replace with actual dataset\n",
        "labels = [1, 0]  # Example labels\n",
        "\n",
        "# Tokenize and split\n",
        "encodings = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    encodings['input_ids'], labels, test_size=0.2, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5XZjPOFTRWw",
        "outputId": "fcbb15c1-d5ae-4dae-85b3-755fac7825e6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchmetrics import Accuracy\n",
        "import torch.nn as nn\n",
        "\n",
        "# Prepare data\n",
        "train_dataset = TensorDataset(train_texts, torch.tensor(train_labels))\n",
        "val_dataset = TensorDataset(val_texts, torch.tensor(val_labels))\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "# Load model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
        "\n",
        "# Training setup\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the accuracy metric for a binary classification task\n",
        "metric = Accuracy(task=\"binary\").to(device)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(50):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        input_ids, labels = [b.to(device) for b in batch]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids)\n",
        "        loss = criterion(outputs.logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEZPHxCITRY-",
        "outputId": "6dd9c0dd-4323-40e0-f882-73f59ac08cc8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.2349765300750732\n",
            "Epoch 2, Loss: 0.6395505666732788\n",
            "Epoch 3, Loss: 0.5692559480667114\n",
            "Epoch 4, Loss: 0.2551383972167969\n",
            "Epoch 5, Loss: 0.16202670335769653\n",
            "Epoch 6, Loss: 0.11991899460554123\n",
            "Epoch 7, Loss: 0.1319114714860916\n",
            "Epoch 8, Loss: 0.08415491878986359\n",
            "Epoch 9, Loss: 0.06294127553701401\n",
            "Epoch 10, Loss: 0.10745317488908768\n",
            "Epoch 11, Loss: 0.08482898771762848\n",
            "Epoch 12, Loss: 0.07753069698810577\n",
            "Epoch 13, Loss: 0.09774982184171677\n",
            "Epoch 14, Loss: 0.10793442279100418\n",
            "Epoch 15, Loss: 0.0872984305024147\n",
            "Epoch 16, Loss: 0.040963053703308105\n",
            "Epoch 17, Loss: 0.045471735298633575\n",
            "Epoch 18, Loss: 0.02772335149347782\n",
            "Epoch 19, Loss: 0.04151534289121628\n",
            "Epoch 20, Loss: 0.030503900721669197\n",
            "Epoch 21, Loss: 0.02515443228185177\n",
            "Epoch 22, Loss: 0.020524965599179268\n",
            "Epoch 23, Loss: 0.030639639124274254\n",
            "Epoch 24, Loss: 0.02565556950867176\n",
            "Epoch 25, Loss: 0.01803230307996273\n",
            "Epoch 26, Loss: 0.02105235680937767\n",
            "Epoch 27, Loss: 0.010230128653347492\n",
            "Epoch 28, Loss: 0.014894470572471619\n",
            "Epoch 29, Loss: 0.015882063657045364\n",
            "Epoch 30, Loss: 0.008385921828448772\n",
            "Epoch 31, Loss: 0.01334073394536972\n",
            "Epoch 32, Loss: 0.012617874890565872\n",
            "Epoch 33, Loss: 0.014695143327116966\n",
            "Epoch 34, Loss: 0.010802363976836205\n",
            "Epoch 35, Loss: 0.01322180312126875\n",
            "Epoch 36, Loss: 0.0134373027831316\n",
            "Epoch 37, Loss: 0.01162269152700901\n",
            "Epoch 38, Loss: 0.015034457668662071\n",
            "Epoch 39, Loss: 0.01235415879637003\n",
            "Epoch 40, Loss: 0.010724056512117386\n",
            "Epoch 41, Loss: 0.011102329008281231\n",
            "Epoch 42, Loss: 0.007610138971358538\n",
            "Epoch 43, Loss: 0.006623812951147556\n",
            "Epoch 44, Loss: 0.011806727387011051\n",
            "Epoch 45, Loss: 0.007277526892721653\n",
            "Epoch 46, Loss: 0.00657596904784441\n",
            "Epoch 47, Loss: 0.006981265731155872\n",
            "Epoch 48, Loss: 0.007779774721711874\n",
            "Epoch 49, Loss: 0.008594665676355362\n",
            "Epoch 50, Loss: 0.007112777326256037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiHeadDiffAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadDiffAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        # Linear transformations for query, key, and value\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Output layer\n",
        "        self.dense = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the input tensor into multiple heads.\"\"\"\n",
        "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
        "        return x.permute(0, 2, 1, 3)  # Shape: (batch_size, num_heads, seq_len, depth)\n",
        "\n",
        "    def attention(self, query, key, value, mask=None):\n",
        "        \"\"\"Compute the scaled dot-product attention.\"\"\"\n",
        "        matmul_qk = torch.matmul(query, key.transpose(-2, -1))  # (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "        # Scale matmul_qk\n",
        "        dk = query.size()[-1]\n",
        "        scaled_attention_logits = matmul_qk / torch.sqrt(torch.tensor(dk, dtype=torch.float32))\n",
        "\n",
        "        # Apply mask (if any)\n",
        "        if mask is not None:\n",
        "            # Expand the mask to match the dimensions of scaled_attention_logits\n",
        "            mask = mask[:, None, None, :].repeat(1, self.num_heads, scaled_attention_logits.size(2), 1)\n",
        "            scaled_attention_logits += (mask * -1e9)  # Large negative number for padding positions\n",
        "\n",
        "        attention_weights = F.softmax(scaled_attention_logits, dim=-1)  # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "        # Compute the attention output\n",
        "        output = torch.matmul(attention_weights, value)  # Shape: (batch_size, num_heads, seq_len, depth)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        # Compute query, key, value projections\n",
        "        query = self.split_heads(self.query(input_ids), batch_size)\n",
        "        key = self.split_heads(self.key(input_ids), batch_size)\n",
        "        value = self.split_heads(self.value(input_ids), batch_size)\n",
        "\n",
        "        # Compute attention\n",
        "        output, attention_weights = self.attention(query, key, value, mask=attention_mask)\n",
        "\n",
        "        # Concatenate heads and pass through the output layer\n",
        "        output = output.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)  # Shape: (batch_size, seq_len, d_model)\n",
        "        output = self.dense(output)\n",
        "\n",
        "        return output, attention_weights\n"
      ],
      "metadata": {
        "id": "kgPlUglTTRbO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "class CustomBERTWithAttention(nn.Module):\n",
        "    def __init__(self, original_model, attention_layer):\n",
        "        super(CustomBERTWithAttention, self).__init__()\n",
        "        self.bert = original_model.bert  # BERT base model\n",
        "        self.attention = attention_layer  # Custom attention layer\n",
        "        self.classifier = original_model.classifier  # Classifier for downstream tasks\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # Get BERT outputs\n",
        "        bert_outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = bert_outputs[0]  # Shape: (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        # Apply custom attention\n",
        "        attention_output, attention_weights = self.attention(sequence_output, attention_mask)\n",
        "\n",
        "        # Apply the classifier to the output of the attention layer\n",
        "        logits = self.classifier(attention_output[:, 0, :])  # Use [CLS] token's output for classification\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "H2iqqVM8TRdK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the paths to your folders\n",
        "folders = ['Akhbarona', 'Arabiya', 'Khaleej', 'SANAD_SUBSET']\n",
        "\n",
        "# Initialize empty lists for texts and labels\n",
        "texts = []\n",
        "labels = []\n",
        "\n",
        "# Number of files to sample from each folder\n",
        "sample_size = 20  # Adjust this number based on how small you want the sample to be\n",
        "\n",
        "# Loop through each folder and load a small sample of text files\n",
        "for folder in folders:\n",
        "    folder_path = os.path.join('/content/drive/MyDrive/Arabic_Sentiment_Tweets/', folder)  # Replace with your actual data path\n",
        "    for subfile in os.listdir(folder_path):\n",
        "        subfile_path = os.path.join(folder_path, subfile)\n",
        "        if os.path.isdir(subfile_path):  # Check if it is a subfolder\n",
        "            text_files = [f for f in os.listdir(subfile_path) if f.endswith('.txt')]\n",
        "            sampled_files = random.sample(text_files, min(len(text_files), sample_size))  # Sample up to `sample_size` files\n",
        "            for text_file in sampled_files:\n",
        "                text_file_path = os.path.join(subfile_path, text_file)\n",
        "                with open(text_file_path, 'r', encoding='utf-8') as file:\n",
        "                    text = file.read()  # Read the content of the text file\n",
        "                    texts.append(text)\n",
        "                    labels.append(folder)  # Use the folder name as the label\n",
        "\n",
        "# Check the lengths of your lists\n",
        "print(len(texts), len(labels))\n",
        "\n",
        "# Split the dataset into train and validation sets (using 20% for validation)\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
        "\n",
        "# Check the split\n",
        "print(len(train_texts), len(val_texts), len(train_labels), len(val_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Smgi0s8kTRnt",
        "outputId": "0ccd43a1-7d11-4261-f4e8-f920084a9633"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200 200\n",
            "160 40 160 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "\n",
        "# Initialize the tokenizer (assuming you're using BERT)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the texts for both training and validation sets\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
        "\n",
        "# Convert labels to tensor format\n",
        "label_mapping = {'Akhbarona': 0, 'Arabiya': 1, 'Khaleej': 2, 'SANAD_SUBSET': 3}\n",
        "train_labels = torch.tensor([label_mapping[label] for label in train_labels])\n",
        "val_labels = torch.tensor([label_mapping[label] for label in val_labels])\n",
        "\n",
        "# Create PyTorch dataset\n",
        "class NewsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Create datasets for DataLoader\n",
        "train_dataset = NewsDataset(train_encodings, train_labels)\n",
        "val_dataset = NewsDataset(val_encodings, val_labels)\n",
        "\n",
        "# Create DataLoaders for training and validation\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
      ],
      "metadata": {
        "id": "3CqVH53Mgarz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, AdamW\n",
        "\n",
        "# Initialize model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)  # 4 classes in your dataset\n",
        "model.to(device)\n",
        "\n",
        "# Set optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(30):  # Train for 3 epochs\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()  # Reset gradients from previous step\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch + 1} complete!')\n",
        "\n",
        "# Save the model after training\n",
        "model.save_pretrained('./fine_tuned_model')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVcm7FcLhUu6",
        "outputId": "f2f29a2b-a1e1-4328-e009-d97b7ca543af"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 complete!\n",
            "Epoch 2 complete!\n",
            "Epoch 3 complete!\n",
            "Epoch 4 complete!\n",
            "Epoch 5 complete!\n",
            "Epoch 6 complete!\n",
            "Epoch 7 complete!\n",
            "Epoch 8 complete!\n",
            "Epoch 9 complete!\n",
            "Epoch 10 complete!\n",
            "Epoch 11 complete!\n",
            "Epoch 12 complete!\n",
            "Epoch 13 complete!\n",
            "Epoch 14 complete!\n",
            "Epoch 15 complete!\n",
            "Epoch 16 complete!\n",
            "Epoch 17 complete!\n",
            "Epoch 18 complete!\n",
            "Epoch 19 complete!\n",
            "Epoch 20 complete!\n",
            "Epoch 21 complete!\n",
            "Epoch 22 complete!\n",
            "Epoch 23 complete!\n",
            "Epoch 24 complete!\n",
            "Epoch 25 complete!\n",
            "Epoch 26 complete!\n",
            "Epoch 27 complete!\n",
            "Epoch 28 complete!\n",
            "Epoch 29 complete!\n",
            "Epoch 30 complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Get predictions\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        correct += (predictions == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSCGLwdrhXRz",
        "outputId": "6c9c2c71-cafe-40af-d5da-d83740ee00be"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qzoR4OYrkHZ0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}